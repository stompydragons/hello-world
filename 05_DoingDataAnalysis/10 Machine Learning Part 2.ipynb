{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 32) (712,)\n",
      "(179, 32) (179,)\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "processed_data_path = os.path.join(os.getcwd(), 'data', 'processed')\n",
    "read_train_path = os.path.join(processed_data_path, 'train.csv')\n",
    "read_test_path = os.path.join(processed_data_path, 'test.csv')\n",
    "\n",
    "train_df = pd.read_csv(read_train_path, index_col='PassengerId')\n",
    "test_df = pd.read_csv(read_test_path, index_col='PassengerId')\n",
    "df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# this takes all rows and all columns except Survived (i.e. from Age onwards) and creates a matrix of float values\n",
    "X = train_df.loc[:, 'Age':].as_matrix().astype('float')\n",
    "\n",
    "# this creates a flattened one dimensional array (or vector) of floats from the outputs using a numpy function\n",
    "y = train_df['Survived'].ravel()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# performance metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "def get_submission_file(model, filename):\n",
    "    # converting to the matrix\n",
    "    test_X = test_df.as_matrix().astype('float')\n",
    "    # get predictions\n",
    "    predictions = model.predict(test_X)\n",
    "    # submission dataframe\n",
    "    df_submission = pd.DataFrame({'PassengerId': test_df.index, 'Survived': predictions})\n",
    "    # submission file\n",
    "    submission_data_path = os.path.join(os.getcwd(), 'data', 'external')\n",
    "    submission_file_path = os.path.join(submission_data_path, filename)\n",
    "    #write to file\n",
    "    df_submission.to_csv(submission_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "## Underfitting vs Overfitting\n",
    "\n",
    "The model needs to be able to make predictions using inputs it hasn't seen before.\n",
    "\n",
    "Underfitting is a model that has been unable to learn a pattern even with the training data and makes many misclassifications, resulting in poor performance. The model is too simple and a more complex model needs to be chosen.\n",
    "\n",
    "However, you have to be careful when chosing a more complext model. If it creates an incredibly complex decision boundary that perfectly separates your classes, it may simply be doing that by memorising the training data and it won't be able to make accurate predictions with new, unseen inputs. This is called overfitting the model because it has poor generalisation.\n",
    "\n",
    "Generalisation is very important to machine learning, because the model needs to be accurate on unseen data. That means you don't want it to have perfect performance on the training data, because it won't be accurate on unseen data. It's a balancing act!\n",
    "\n",
    "## Regularisation\n",
    "\n",
    "This is a way of tackling overfitting by reducing the complexity of a model in order to make it more balanced.\n",
    "\n",
    "When we created the logicistic regression model, the we set the random_state parameter but didn't touch any of the others, which can all be used to tune the model in various ways. The C parameter is the regularisation parameter. The default is C=1.0.\n",
    "\n",
    "If you increase the value of C a lot, the model will be more complex and the model will be overfitted. If you decrease the value of C, the model will be simpler and may be underfitted.\n",
    "\n",
    "Penalty is another term for regularisation, and the default is penalty=l2. The common settings for penalty ate L1 and L2 and these can be changed to over or underfit your model.\n",
    "\n",
    "The various parameter that can be set in a model to fine-tune it are often called **hyperparameters**.\n",
    "\n",
    "## Hyperparameter Optimisation\n",
    "\n",
    "The most popular hyperparameter optimisation technique is called grid search. First create a grid of values for hyperparameters you want to try (for example, if you want to try three different settings for one parameter and two different settings for another, you'd have a 3x2 grid with 6 cells, then evaluate the performance for each combination in a model. You can then select the best combination to use in your model.\n",
    "\n",
    "To evaluate the performance, you can use the train-test split as before, but for hyperparameter optmimisation there is a technique called **cross-validation** that can be used instead.\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "The data is split into three different parts, instead of two: training, test, cross-validation.\n",
    "\n",
    "Pass the training data to the model to get the trained model. Then evaluate the performance of the model on the cross-validation dataset.\n",
    "\n",
    "So for each hyperparameter set, you build a model, train it, and evaluate on the cross-validation data. Then put the score into the grid and find the model with the best score.\n",
    "\n",
    "Finally, pass the test data to the best model to evaluate the final score for the model. In other words, the training and cross-validation data is used multiple times to get the best model, and the test data is used only once to evaluate the final model.\n",
    "\n",
    "## K-fold Cross-validation\n",
    "\n",
    "This is a more advanced method of getting cross-validation data than was described in the previous section, where the split is always the same. 3-Fold cross-validation is a common use.\n",
    "\n",
    "Split the training data equally into three portions. For ease, we'll call them portions 1, 2, and 3. In the first iteration, portions 2 and 3 are used for training and portion 1 is the cross-validation data. The score is recorded. In the second iteration, portions 1 and 3 are used for training and portion 2 is the cross-validation. The third iteration uses 1 and 2 for training and 3 for validation. \n",
    "\n",
    "At the end, you'll have three scores and you'll take the mean of those scores as your overall score for this iteration of the model and its hyperparameters. It's useful to take the standard deviation, too, to make sure there isn't an unacceptable deviation in the scores that might indicate a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning in practice\n",
    "\n",
    "## Hyperparameter optimisation\n",
    "\n",
    "There is a scikit learn function, thankfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base model\n",
    "model_lr = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary listing out the hyperparameters that will be tried\n",
    "parameters = {'C': [1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# create the grid search, setting cv to 3 performs the 3-fold crossvalidation\n",
    "clf = GridSearchCV(model_lr, param_grid=parameters, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the grid search is set up, the model can be trained with the different hyperparameter combinations\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now find which was the best combination\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.83\n"
     ]
    }
   ],
   "source": [
    "print('best score: {0:.2f}'.format(clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to get close to the maximum we will see from a logistic regression model, so the improvement is only very small over the original logistic regression model from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for logistic regression model - version 2: 0.82682\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test data, internally it will use the best model to get the best score\n",
    "print('score for logistic regression model - version 2: {0:.5f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get submission file\n",
    "get_submission_file(clf, '03_lr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Normalisation and Standardisation\n",
    "\n",
    "For many ML algorithms, model performance is improved if the features are on the same scale. For the kind of logistic regression model used in the course it may not make much difference, but it's something to explore with other models.\n",
    "\n",
    "For example, in the Titantic dataset there are Age, Fare, and FamilySize features which all have very different ranges and scales. Ideally, they should all have the same scale.\n",
    "\n",
    "Scales of 0 to 1 or -1 to +1 are common, but don't use scales with minus values for features with negative values make no sense. So for Age, Fare, and FamilySize, a scale of 0 to 1 makes sense. The idea is to map the data for each feature to a scale like this (use the same scale for all features, obviously).\n",
    "\n",
    "The other technique is feature standardisation, which is about the distribution of the data in a feature. So for Age, Fare, and FamilySize, they might all have different means and standard distributions. The aim is to get the features to have a mean of 0.0 and a variance of 1.0\n",
    "\n",
    "Thankfully, scikit-learn also provides functions for doing this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature normalisation\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled[:,0].min(), X_train_scaled[:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalise test data\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model with standardisation and hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base model\n",
    "model_lr2 = LogisticRegression(random_state=0)\n",
    "\n",
    "# create a dictionary listing out the hyperparameters that will be tried\n",
    "parameters = {'C': [1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# create the grid search, setting cv to 3 performs the 3-fold crossvalidation\n",
    "clf2 = GridSearchCV(model_lr2, param_grid=parameters, cv=3)\n",
    "\n",
    "# now the grid search is set up, the model can be trained with the different hyperparameter combinations\n",
    "clf2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now find which was the best combination\n",
    "clf2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.813202\n"
     ]
    }
   ],
   "source": [
    "print('best score: {0:.6f}'.format(clf2.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for logistic regression model - version 3: 0.81564\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test data, internally it will use the best model to get the best score\n",
    "print('score for logistic regression model - version 3: {0:.5f}'.format(clf2.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is not an improvement, so I'm not going to submit this to Kaggle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model persistence\n",
    "\n",
    "This is the process of saving the trained model to the disk so it can be reloaded and reused without needing to be retrained.\n",
    "\n",
    "This means you can share the model with others without having to share the training steps and data. You can also use the persistence to create a machine learning API on top of it.\n",
    "\n",
    "## Pickle!\n",
    "\n",
    "The pickle library can be used to save the trained model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model paths\n",
    "model_lr_file_path = os.path.join(os.getcwd(), 'models', 'lr_model.pkl')\n",
    "model_lr2_file_path = os.path.join(os.getcwd(), 'models', 'lr2_model.pkl')\n",
    "scalar_file_path = os.path.join(os.getcwd(), 'models', 'scalar.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open the files to write\n",
    "model_lr_pickle = open(model_lr_file_path, 'wb')\n",
    "model_lr2_pickle = open(model_lr2_file_path, 'wb')\n",
    "scalar_pickle = open(scalar_file_path, 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# persist the data\n",
    "pickle.dump(clf, model_lr_pickle)\n",
    "pickle.dump(clf2, model_lr2_pickle)\n",
    "pickle.dump(scaler, scalar_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close the files\n",
    "model_lr_pickle.close()\n",
    "model_lr2_pickle.close()\n",
    "scalar_pickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the persisted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the files\n",
    "model_lr_pickle = open(model_lr_file_path, 'rb')\n",
    "model_lr2_pickle = open(model_lr2_file_path, 'rb')\n",
    "scalar_pickle = open(scalar_file_path, 'rb')\n",
    "#load files\n",
    "clf_loaded = pickle.load(model_lr_pickle)\n",
    "clf2_loaded = pickle.load(model_lr2_pickle)\n",
    "scalar_loaded = pickle.load(scalar_pickle)\n",
    "# close the files\n",
    "model_lr_pickle.close()\n",
    "model_lr2_pickle.close()\n",
    "scalar_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for logistic regression model 3: 0.815642\n"
     ]
    }
   ],
   "source": [
    "# transform the test data using the loaded scalar\n",
    "X_test_scaled = scalar_loaded.transform(X_test)\n",
    "# calculate the score using the loaded model\n",
    "print('score for logistic regression model 3: {0:5f}'.format(clf2_loaded.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
